{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization – Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization is most useful when building deep neural networks. To demonstrate this, we'll create a convolutional neural network with 20 convolutional layers, followed by a fully connected layer. We'll use it to classify handwritten digits in the MNIST dataset, which should be familiar to you by now.\n",
    "\n",
    "This is **not** a good network for classfying MNIST digits. You could create a _much_ simpler network and get _better_ results. However, to give you hands-on experience with batch normalization, we had to make an example that was:\n",
    "1. Complicated enough that training would benefit from batch normalization.\n",
    "2. Simple enough that it would train quickly, since this is meant to be a short exercise just to give you some practice adding batch normalization.\n",
    "3. Simple enough that the architecture would be easy to understand without additional resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes two versions of the network that you can edit. The first uses higher level functions from the `tf.layers` package. The second is the same network, but uses only lower level functions in the `tf.nn` package.\n",
    "\n",
    "1. [Batch Normalization with `tf.layers.batch_normalization`](#example_1)\n",
    "2. [Batch Normalization with `tf.nn.batch_normalization`](#example_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads TensorFlow, downloads the MNIST dataset if necessary, and loads it into an object named `mnist`. You'll need to run this cell before running anything else in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization using `tf.layers.batch_normalization`<a id=\"example_1\"></a>\n",
    "\n",
    "This version of the network uses `tf.layers` for almost everything, and expects you to implement batch normalization using [`tf.layers.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following function to create fully connected layers in our network. We'll create them with the specified number of neurons and a ReLU activation function.\n",
    "\n",
    "This version of the function does not include batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL\n",
    "\"\"\"\n",
    "def fully_connected(prev_layer, num_units):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "    layer = tf.layers.dense(prev_layer, num_units, activation=tf.nn.relu)\n",
    "    #tf.layers.dense : Functional interface for the densely-connected layer.\n",
    "    #tf.layers.dense를 사용하면 간단하게 신경망을 만들 수 있다. 파라미터는 (인풋, 출력 차원(아웃풋 사이즈)). 나머지는 모두 optional. Relu를 활성화함수로 FC\n",
    "    #https://www.tensorflow.org/api_docs/python/tf/layers/dense\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following function to create convolutional layers in our network. They are very basic: we're always using a 3x3 kernel, ReLU activation functions, strides of 1x1 on layers with odd depths, and strides of 2x2 on layers with even depths. We aren't bothering with pooling layers at all in this network.\n",
    "\n",
    "This version of the function does not include batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL\n",
    "\"\"\"\n",
    "def conv_layer(prev_layer, layer_depth):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1 #커널 stride : 홀수에서는 1, 짝수에서는 2\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', activation=tf.nn.relu) #CNN\n",
    "    #tf.layers.conv2d : (인풋, 필터, 커널 사이즈, 커널 스트라이드). 나머지는 optional\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the following cell**, along with the earlier cells (to load the dataset and define the necessary functions). \n",
    "\n",
    "This cell builds the network **without** batch normalization, then trains it on the MNIST dataset. It displays loss and accuracy data periodically while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69085, Validation accuracy: 0.09240\n",
      "Batch: 25: Training loss: 0.37414, Training accuracy: 0.07812\n",
      "Batch: 50: Training loss: 0.32378, Training accuracy: 0.15625\n",
      "Batch: 75: Training loss: 0.32661, Training accuracy: 0.12500\n",
      "Batch: 100: Validation loss: 0.32560, Validation accuracy: 0.09860\n",
      "Batch: 125: Training loss: 0.32542, Training accuracy: 0.10938\n",
      "Batch: 150: Training loss: 0.32473, Training accuracy: 0.09375\n",
      "Batch: 175: Training loss: 0.32720, Training accuracy: 0.07812\n",
      "Batch: 200: Validation loss: 0.32601, Validation accuracy: 0.09760\n",
      "Batch: 225: Training loss: 0.32588, Training accuracy: 0.07812\n",
      "Batch: 250: Training loss: 0.32769, Training accuracy: 0.10938\n",
      "Batch: 275: Training loss: 0.32391, Training accuracy: 0.10938\n",
      "Batch: 300: Validation loss: 0.32585, Validation accuracy: 0.09760\n",
      "Batch: 325: Training loss: 0.32699, Training accuracy: 0.09375\n",
      "Batch: 350: Training loss: 0.32357, Training accuracy: 0.14062\n",
      "Batch: 375: Training loss: 0.32501, Training accuracy: 0.07812\n",
      "Batch: 400: Validation loss: 0.32593, Validation accuracy: 0.09240\n",
      "Batch: 425: Training loss: 0.32686, Training accuracy: 0.06250\n",
      "Batch: 450: Training loss: 0.32568, Training accuracy: 0.09375\n",
      "Batch: 475: Training loss: 0.32672, Training accuracy: 0.09375\n",
      "Batch: 500: Validation loss: 0.32664, Validation accuracy: 0.09240\n",
      "Batch: 525: Training loss: 0.32392, Training accuracy: 0.15625\n",
      "Batch: 550: Training loss: 0.33082, Training accuracy: 0.06250\n",
      "Batch: 575: Training loss: 0.32421, Training accuracy: 0.14062\n",
      "Batch: 600: Validation loss: 0.32511, Validation accuracy: 0.11260\n",
      "Batch: 625: Training loss: 0.32725, Training accuracy: 0.09375\n",
      "Batch: 650: Training loss: 0.32508, Training accuracy: 0.07812\n",
      "Batch: 675: Training loss: 0.32423, Training accuracy: 0.12500\n",
      "Batch: 700: Validation loss: 0.32517, Validation accuracy: 0.11260\n",
      "Batch: 725: Training loss: 0.32487, Training accuracy: 0.09375\n",
      "Batch: 750: Training loss: 0.32462, Training accuracy: 0.10938\n",
      "Batch: 775: Training loss: 0.32370, Training accuracy: 0.14062\n",
      "Final validation accuracy: 0.11260\n",
      "Final test accuracy: 0.11350\n",
      "Accuracy on 100 samples: 0.14\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL\n",
    "\"\"\"\n",
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    #Variable 노드는 다른 언어의 Variable과 달리, 실행 시 텐서플로우가 변경시키는 값이라고 생각하면 이해하기 편하다.(학습하는 과정에서 변경시킨다.)\n",
    "    #tf.placeholder()는 입력 데이터를 만들 때 주로 사용한다. (실제 훈련 예제를 제공하는 변수) - 초기값을 지정할 필요 없다. (모델 입력시 변경되지 않을 데이터)\n",
    "    #tf.Variable()은 데이터의 상태를 저장할 때 주로 사용한다. (가중치나 편향 등의 학습 가능한 변수) - 초기값을 지정해야 한다. (학습 되는 데이터)\n",
    "    #http://stackoverflow.com/questions/36693740/whats-the-difference-between-tf-placeholder-and-tf-variable\n",
    "    \n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20): #CNN 레이어 20개 생성\n",
    "        layer = conv_layer(layer, layer_i) #이전 CNN 레이어와 연결\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list() #FC 연결하기 전에 평탄화 시켜야 한다.\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]]) #FC 인풋에 맞도록 reshape\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100) #FC 연결. 활성화 함수는 Relu\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10) #손실과 정확도 계산하기 위한 마지막 레이어 연결. 활성화 함수가 없다.\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    #sigmoid_cross_entropy_with_logits로 손실 계산. logits과 labels 필요.\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    #Adam으로 최적화. 학습률과 손실 필요.\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1)) #정확도 계산 위해 일치 여부 확인\n",
    "    #tf.equal로 인덱스가 일치하면 제대로 판별된 것\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #정확도 계산\n",
    "    #correct_prediction 텐서의 평균을 구하면 정확도가 된다.\n",
    "    #correct_prediction에는 1(맞음, True), 0(틀림, False)의 값들만 있으므로 float32로 평균을 게산하면 정확도가 된다.\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer()) #초기화\n",
    "        for batch_i in range(num_batches): #배치 사이즈 만큼 반복하면서\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size) #배치 사이즈만큼 인풋 데이터와 정답 레이블을 가져온다.\n",
    "            #******************** 여기서는 Batch Normalization을 진행하지 않았다. 단순히 배치 사이즈만큼 잘라서 가져온 것 뿐 ********************\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys}) #학습 실행\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0: #100번마다 표시\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0: #25번 마다 표시\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        #학습 후 최종적으로 표시\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually. This won't work if batch normalization isn't implemented correctly.\n",
    "        correct = 0\n",
    "        for i in range(100): #100개 이미지에 대한 개별 점수\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]]})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100) #Batch Normalization이 적용되지 않았으므로 정확도가 높지 않다. (0.14)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph() #그래프 초기화\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this many layers, it's going to take a lot of iterations for this network to learn. By the time you're done training these 800 batches, your final test and validation accuracies probably won't be much better than 10%. (It will be different each time, but will most likely be less than 15%.)\n",
    "\n",
    "Using batch normalization, you'll be able to train this same network to over 90% in that same number of batches.\n",
    "\n",
    "\n",
    "# Add batch normalization\n",
    "\n",
    "We've copied the previous three cells to get you started. **Edit these cells** to add batch normalization to the network. For this exercise, you should use [`tf.layers.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization) to handle most of the math, but you'll need to make a few other changes to your network to integrate batch normalization. You may want to refer back to the lesson notebook to remind yourself of important things, like how your graph operations need to know whether or not you are performing training or inference. \n",
    "\n",
    "If you get stuck, you can check out the `Batch_Normalization_Solutions` notebook to see how we did things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Modify `fully_connected` to add batch normalization to the fully connected layers it creates. Feel free to change the function's parameters if it helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :param is_training: bool or Tensor\n",
    "        Indicates whether or not the network is currently training, which tells the batch normalization\n",
    "        layer whether or not it should update or use its population statistics.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "    #tf.layers.dense : Functional interface for the densely-connected layer.\n",
    "    #tf.layers.dense를 사용하면 간단하게 신경망을 만들 수 있다. 파라미터는 (인풋, 출력 차원(아웃풋 사이즈)). 나머지는 모두 optional. Relu를 활성화함수로 FC\n",
    "    #https://www.tensorflow.org/api_docs/python/tf/layers/dense\n",
    "    layer = tf.layers.batch_normalization(layer, training=is_training) #Batch Normalization 적용\n",
    "    #1. Batch normalization된 레이어에는 평향이 포함되지 않는다.\n",
    "    #2. tf.layers.batch_normalization에게 네트워크가 학습 중인지 여부를 알려줘야 한다.\n",
    "    #3. 활성화 함수를 호출하기 전에 Batch normalization를 추가한다.\n",
    "    #tf.layers.batch_normalization : Batch Normalization 적용. (인풋). training는 optional. 현재 학습인지 테스트인지 bool로.\n",
    "    #Batch Normalization : 각 layer의 input을 normailze하여, distribution을 일정하게 유지시키려는 시도.\n",
    "    #https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\n",
    "    layer = tf.nn.relu(layer) #활성화 함수는 Relu\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Modify `conv_layer` to add batch normalization to the convolutional layers it creates. Feel free to change the function's parameters if it helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :param is_training: bool or Tensor\n",
    "        Indicates whether or not the network is currently training, which tells the batch normalization\n",
    "        layer whether or not it should update or use its population statistics.\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1 #커널 stride : 홀수에서는 1, 짝수에서는 2\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', use_bias=False, activation=None) #CNN\n",
    "    #tf.layers.conv2d : (인풋, 필터, 커널 사이즈, 커널 스트라이드). 나머지는 optional\n",
    "    #Batch normalization를 적용하기 위해서 편향은 제거해 준다.\n",
    "    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "    #1. Batch normalization된 레이어에는 평향이 포함되지 않는다.\n",
    "    #2. tf.layers.batch_normalization에게 네트워크가 학습 중인지 여부를 알려줘야 한다.\n",
    "    #3. 활성화 함수를 호출하기 전에 Batch normalization를 추가한다.\n",
    "    #tf.layers.batch_normalization : Batch Normalization 적용. (인풋). training는 optional. 현재 학습인지 테스트인지 bool로.\n",
    "    #Batch Normalization : 각 layer의 input을 normailze하여, distribution을 일정하게 유지시키려는 시도.\n",
    "    #https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\n",
    "    conv_layer = tf.nn.relu(conv_layer) #활성화 함수는 Relu\n",
    "    \n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Edit the `train` function to support batch normalization. You'll need to make sure the network knows whether or not it is training, and you'll need to make sure it updates and uses its population statistics correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69136, Validation accuracy: 0.11260\n",
      "Batch: 25: Training loss: 0.58744, Training accuracy: 0.12500\n",
      "Batch: 50: Training loss: 0.47918, Training accuracy: 0.10938\n",
      "Batch: 75: Training loss: 0.40694, Training accuracy: 0.12500\n",
      "Batch: 100: Validation loss: 0.36505, Validation accuracy: 0.08680\n",
      "Batch: 125: Training loss: 0.33912, Training accuracy: 0.01562\n",
      "Batch: 150: Training loss: 0.33904, Training accuracy: 0.15625\n",
      "Batch: 175: Training loss: 0.32500, Training accuracy: 0.20312\n",
      "Batch: 200: Validation loss: 0.30870, Validation accuracy: 0.29340\n",
      "Batch: 225: Training loss: 0.28605, Training accuracy: 0.40625\n",
      "Batch: 250: Training loss: 0.22937, Training accuracy: 0.50000\n",
      "Batch: 275: Training loss: 0.11288, Training accuracy: 0.79688\n",
      "Batch: 300: Validation loss: 0.07882, Validation accuracy: 0.87460\n",
      "Batch: 325: Training loss: 0.04931, Training accuracy: 0.92188\n",
      "Batch: 350: Training loss: 0.04819, Training accuracy: 0.90625\n",
      "Batch: 375: Training loss: 0.03044, Training accuracy: 0.92188\n",
      "Batch: 400: Validation loss: 0.07260, Validation accuracy: 0.89400\n",
      "Batch: 425: Training loss: 0.11610, Training accuracy: 0.87500\n",
      "Batch: 450: Training loss: 0.03610, Training accuracy: 0.93750\n",
      "Batch: 475: Training loss: 0.00497, Training accuracy: 0.98438\n",
      "Batch: 500: Validation loss: 0.03232, Validation accuracy: 0.95640\n",
      "Batch: 525: Training loss: 0.01386, Training accuracy: 0.98438\n",
      "Batch: 550: Training loss: 0.02708, Training accuracy: 0.96875\n",
      "Batch: 575: Training loss: 0.01419, Training accuracy: 0.98438\n",
      "Batch: 600: Validation loss: 0.02586, Validation accuracy: 0.96700\n",
      "Batch: 625: Training loss: 0.06789, Training accuracy: 0.89062\n",
      "Batch: 650: Training loss: 0.07069, Training accuracy: 0.92188\n",
      "Batch: 675: Training loss: 0.07816, Training accuracy: 0.89062\n",
      "Batch: 700: Validation loss: 0.01659, Validation accuracy: 0.97880\n",
      "Batch: 725: Training loss: 0.01371, Training accuracy: 0.96875\n",
      "Batch: 750: Training loss: 0.00302, Training accuracy: 1.00000\n",
      "Batch: 775: Training loss: 0.02341, Training accuracy: 0.95312\n",
      "Final validation accuracy: 0.97320\n",
      "Final test accuracy: 0.97530\n",
      "Accuracy on 100 samples: 1.0\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    #Variable 노드는 다른 언어의 Variable과 달리, 실행 시 텐서플로우가 변경시키는 값이라고 생각하면 이해하기 편하다.(학습하는 과정에서 변경시킨다.)\n",
    "    #tf.placeholder()는 입력 데이터를 만들 때 주로 사용한다. (실제 훈련 예제를 제공하는 변수) - 초기값을 지정할 필요 없다. (모델 입력시 변경되지 않을 데이터)\n",
    "    #tf.Variable()은 데이터의 상태를 저장할 때 주로 사용한다. (가중치나 편향 등의 학습 가능한 변수) - 초기값을 지정해야 한다. (학습 되는 데이터)\n",
    "    #http://stackoverflow.com/questions/36693740/whats-the-difference-between-tf-placeholder-and-tf-variable\n",
    "    \n",
    "    # Add placeholder to indicate whether or not we're training the model\n",
    "    is_training = tf.placeholder(tf.bool) #Batch Normaliztion은 트레이닝과 테스트 시 적용 여부를 달리해야 하므로 이를 알려줄 텐서가 필요하다.\n",
    "    \n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20): #CNN 레이어 20개 생성\n",
    "        layer = conv_layer(layer, layer_i, is_training) #이전 CNN 레이어와 연결. 학습 중인지 추론 중인지 여부\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list() #FC 연결하기 전에 평탄화 시켜야 한다.\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]]) #FC 인풋에 맞도록 reshape\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100, is_training) #FC 연결. 활성화 함수는 Relu\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10) #손실과 정확도 계산하기 위한 마지막 레이어 연결. 활성화 함수가 없다.\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    #sigmoid_cross_entropy_with_logits로 손실 계산. logits과 labels 필요.\n",
    "\n",
    "    # Tell TensorFlow to update the population statistics while training\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): #의존성 지정\n",
    "        #tf.layers.batch_normalization에 전달하는 학습 매개 변수를 연결한다. \n",
    "        #이 행이 없으면 TensorFlow의 배치 정규화 레이어가 테스트(추측)중에 올바르게 작동하지 않는다.\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "        #Adam으로 최적화. 학습률과 손실 필요.\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1)) #정확도 계산 위해 일치 여부 확인\n",
    "    #tf.equal로 인덱스가 일치하면 제대로 판별된 것\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #정확도 계산\n",
    "    #correct_prediction 텐서의 평균을 구하면 정확도가 된다.\n",
    "    #correct_prediction에는 1(맞음, True), 0(틀림, False)의 값들만 있으므로 float32로 평균을 게산하면 정확도가 된다.\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer()) #초기화\n",
    "        for batch_i in range(num_batches): #배치 사이즈 만큼 반복하면서\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size) #배치 사이즈만큼 인풋 데이터와 정답 레이블을 가져온다.\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True}) #학습 실행\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0: #100번마다 표시\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels,\n",
    "                                                              is_training: False})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0: #25번 마다 표시\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        #학습 후 최종적으로 표시\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually. This won't work if batch normalization isn't implemented correctly.\n",
    "        correct = 0\n",
    "        for i in range(100): #100개 이미지에 대한 개별 점수\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    is_training: False})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100) #Batch Normalization이 적용되어 정확도가 높아 졌다. (0.14 -> 1.0)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph() #그래프 초기화\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With batch normalization, you should now get an accuracy over 90%. Notice also the last line of the output: `Accuracy on 100 samples`. If this value is low while everything else looks good, that means you did not implement batch normalization correctly. Specifically, it means you either did not calculate the population mean and variance while training, or you are not using those values during inference.\n",
    "\n",
    "# Batch Normalization using `tf.nn.batch_normalization`<a id=\"example_2\"></a>\n",
    "\n",
    "Most of the time you will be able to use higher level functions exclusively, but sometimes you may want to work at a lower level. For example, if you ever want to implement a new feature – something new enough that TensorFlow does not already include a high-level implementation of it, like batch normalization in an LSTM – then you may need to know these sorts of things.\n",
    "\n",
    "This version of the network uses `tf.nn` for almost everything, and expects you to implement batch normalization using [`tf.nn.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization).\n",
    "\n",
    "**Optional TODO:** You can run the next three cells before you edit them just to see how the network performs without batch normalization. However, the results should be pretty much the same as you saw with the previous example before you added batch normalization. \n",
    "\n",
    "**TODO:** Modify `fully_connected` to add batch normalization to the fully connected layers it creates. Feel free to change the function's parameters if it helps.\n",
    "\n",
    "**Note:** For convenience, we continue to use `tf.layers.dense` for the `fully_connected` layer. By this point in the class, you should have no problem replacing that with matrix operations between the `prev_layer` and explicit weights and biases variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :param is_training: bool or Tensor\n",
    "        Indicates whether or not the network is currently training, which tells the batch normalization\n",
    "        layer whether or not it should update or use its population statistics.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "\n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "\n",
    "    gamma = tf.Variable(tf.ones([num_units]))\n",
    "    beta = tf.Variable(tf.zeros([num_units]))\n",
    "\n",
    "    pop_mean = tf.Variable(tf.zeros([num_units]), trainable=False)\n",
    "    pop_variance = tf.Variable(tf.ones([num_units]), trainable=False)\n",
    "\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    def batch_norm_training():\n",
    "        batch_mean, batch_variance = tf.nn.moments(layer, [0])\n",
    "\n",
    "        decay = 0.99\n",
    "        train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_variance = tf.assign(pop_variance, pop_variance * decay + batch_variance * (1 - decay))\n",
    "\n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_normalization(layer, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    " \n",
    "    def batch_norm_inference():\n",
    "        return tf.nn.batch_normalization(layer, pop_mean, pop_variance, beta, gamma, epsilon)\n",
    "\n",
    "    batch_normalized_output = tf.cond(is_training, batch_norm_training, batch_norm_inference)\n",
    "    return tf.nn.relu(batch_normalized_output)\n",
    "\n",
    "#tf.nn.batch_normalization를 사용해서도 Batch Normalization을 구현할 수 있다. 이전보다 더 복잡."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Modify `conv_layer` to add batch normalization to the fully connected layers it creates. Feel free to change the function's parameters if it helps.\n",
    "\n",
    "**Note:** Unlike in the previous example that used `tf.layers`, adding batch normalization to these convolutional layers _does_ require some slight differences to what you did in `fully_connected`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :param is_training: bool or Tensor\n",
    "        Indicates whether or not the network is currently training, which tells the batch normalization\n",
    "        layer whether or not it should update or use its population statistics.\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    \n",
    "    in_channels = prev_layer.get_shape().as_list()[3]\n",
    "    out_channels = layer_depth*4\n",
    "    \n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([3, 3, in_channels, out_channels], stddev=0.05))\n",
    "    \n",
    "    layer = tf.nn.conv2d(prev_layer, weights, strides=[1,strides, strides, 1], padding='SAME')\n",
    "\n",
    "    gamma = tf.Variable(tf.ones([out_channels]))\n",
    "    beta = tf.Variable(tf.zeros([out_channels]))\n",
    "\n",
    "    pop_mean = tf.Variable(tf.zeros([out_channels]), trainable=False)\n",
    "    pop_variance = tf.Variable(tf.ones([out_channels]), trainable=False)\n",
    "\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    def batch_norm_training():\n",
    "        # Important to use the correct dimensions here to ensure the mean and variance are calculated \n",
    "        # per feature map instead of for the entire layer\n",
    "        batch_mean, batch_variance = tf.nn.moments(layer, [0,1,2], keep_dims=False)\n",
    "\n",
    "        decay = 0.99\n",
    "        train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_variance = tf.assign(pop_variance, pop_variance * decay + batch_variance * (1 - decay))\n",
    "\n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_normalization(layer, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    " \n",
    "    def batch_norm_inference():\n",
    "        return tf.nn.batch_normalization(layer, pop_mean, pop_variance, beta, gamma, epsilon)\n",
    "\n",
    "    batch_normalized_output = tf.cond(is_training, batch_norm_training, batch_norm_inference)\n",
    "    return tf.nn.relu(batch_normalized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Edit the `train` function to support batch normalization. You'll need to make sure the network knows whether or not it is training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69088, Validation accuracy: 0.10020\n",
      "Batch: 25: Training loss: 0.58359, Training accuracy: 0.10938\n",
      "Batch: 50: Training loss: 0.47969, Training accuracy: 0.10938\n",
      "Batch: 75: Training loss: 0.40483, Training accuracy: 0.04688\n",
      "Batch: 100: Validation loss: 0.36213, Validation accuracy: 0.08680\n",
      "Batch: 125: Training loss: 0.35079, Training accuracy: 0.06250\n",
      "Batch: 150: Training loss: 0.35882, Training accuracy: 0.01562\n",
      "Batch: 175: Training loss: 0.38920, Training accuracy: 0.04688\n",
      "Batch: 200: Validation loss: 0.37116, Validation accuracy: 0.08680\n",
      "Batch: 225: Training loss: 0.39730, Training accuracy: 0.10938\n",
      "Batch: 250: Training loss: 0.49332, Training accuracy: 0.07812\n",
      "Batch: 275: Training loss: 0.39005, Training accuracy: 0.06250\n",
      "Batch: 300: Validation loss: 0.41825, Validation accuracy: 0.11700\n",
      "Batch: 325: Training loss: 0.69535, Training accuracy: 0.14062\n",
      "Batch: 350: Training loss: 0.67622, Training accuracy: 0.04688\n",
      "Batch: 375: Training loss: 0.33344, Training accuracy: 0.39062\n",
      "Batch: 400: Validation loss: 0.20812, Validation accuracy: 0.66740\n",
      "Batch: 425: Training loss: 0.12128, Training accuracy: 0.82812\n",
      "Batch: 450: Training loss: 0.09265, Training accuracy: 0.82812\n",
      "Batch: 475: Training loss: 0.06111, Training accuracy: 0.89062\n",
      "Batch: 500: Validation loss: 0.03366, Validation accuracy: 0.94920\n",
      "Batch: 525: Training loss: 0.00451, Training accuracy: 1.00000\n",
      "Batch: 550: Training loss: 0.01725, Training accuracy: 0.98438\n",
      "Batch: 575: Training loss: 0.09495, Training accuracy: 0.81250\n",
      "Batch: 600: Validation loss: 0.03279, Validation accuracy: 0.95040\n",
      "Batch: 625: Training loss: 0.04176, Training accuracy: 0.93750\n",
      "Batch: 650: Training loss: 0.12405, Training accuracy: 0.87500\n",
      "Batch: 675: Training loss: 0.03568, Training accuracy: 0.95312\n",
      "Batch: 700: Validation loss: 0.02433, Validation accuracy: 0.96320\n",
      "Batch: 725: Training loss: 0.04263, Training accuracy: 0.93750\n",
      "Batch: 750: Training loss: 0.03603, Training accuracy: 0.92188\n",
      "Batch: 775: Training loss: 0.04076, Training accuracy: 0.96875\n",
      "Final validation accuracy: 0.96200\n",
      "Final test accuracy: 0.95880\n",
      "Accuracy on 100 samples: 0.98\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # Add placeholder to indicate whether or not we're training the model\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100, is_training)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels,\n",
    "                                                              is_training: False})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels, \n",
    "                                  is_training: False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually, just to make sure batch normalization really worked\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    is_training: False})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the model with batch normalization should reach an accuracy over 90%. There are plenty of details that can go wrong when implementing at this low level, so if you got it working - great job! If not, do not worry, just look at the `Batch_Normalization_Solutions` notebook to see what went wrong."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
